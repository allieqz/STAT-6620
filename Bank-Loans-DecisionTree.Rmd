Homework 4.1
===
DecisionTree algorithms, C5.0 - Identifying Risky Bank Loans
---
<b>STEP 1: Loading the data</b>
The original dataset is available at the UCI Machine Learning Data Repository. The data was donated by Hans Hofmann of the University of Hamburg, and contains loan information from a credit agency in Germany.The original dataset was modified by the authors of the Machine Learning with R book to eliminate some preprocessing steps.
The dataset contains 1000 observations with 17 variables.
```{r}
# Reading data into R
credit <- read.csv("credit.csv")
```
<b>STEP 2: Exploring and preparing the data</b>
As we can see below, some of the variables are factors, some are numeric (integers). The output variable "Default" has two levels - "yes" and "no".
```{r}
str(credit)
# Looking closely at "Default" (output) variable
table(credit$default)
```
Our next step is to create training and test sets. To perform randomized sampling, we use the function <i>sample()</i>, which utilizes mathematical function called a pseudorandom number generator. Since we have 1000 observations, we allocate 900 observations (90%) to the training set and 100 observations (10%) to the test set. At the end of this step, we want to make sure that proportions of records belonging to the classes "Defaulted"("yes") and "Did Not Default"("no") inside of the training and test sets are very similar to the proportions of the entire dataset.
```{r}
# Setting the seed to get the reproducable results
set.seed(123)
# Identifying the size of the training set
train_sample <- sample(1000, 900)
# Constructing traing and test sets
credit_train <- credit[train_sample, ]
credit_test  <- credit[-train_sample, ]

# Checking the class proportions
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))

```
<b> STEP 3: Training a model on the data</b>
The next step is to build an actual Decision Tree. To be able to do that, we need to install and load the <i>C5.0</i> package. After the training the model, we can take a look at the Tree that we grew. It has 57 decision nodes.
```{r}
library(C50)
credit_model <- C5.0(credit_train[-17], credit_train$default)
summary(credit_model)
```
<b> STEP 4: Evaluating model performance</b>
Next step is to test a model and evaluate the performance looking at the confusion matrix. 
The total Accuracy rate is 73%. Unfortunately, the biggest concern is the high number of False Negatives (19) - those are "Defaulted" cases predicted as "No Default". This is very critical for the model, since those errors are the most costly to the financial organizations.
```{r}
credit_pred <- predict(credit_model, credit_test)

# Displaying the confusion matrix
library(gmodels)
CrossTable(credit_test$default, credit_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```
<b>STEP 5: Improving model performance</b>

<b>Boosting</b>

Adaptive Boosting can be applied to C5.0 algorithm to improve the performance. We can do this by adding <i>trials</i> parameter. 10 trials is a good number to use. Boosting helps to build a stronger classifier with iteratively learning weak classifiers.
```{r}
# Adding trials parameter to the model
credit_boost10 <- C5.0(credit_train[-17], credit_train$default,
                       trials = 10)
credit_boost10

# Testing updated model
credit_boost_pred10 <- predict(credit_boost10, credit_test)
CrossTable(credit_test$default, credit_boost_pred10,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))
```
The size of the new Tree is now smaller. The Accuracy rate of the new model is 88%. Boosting gave us a slight improvement of the accuracy. It also reduced the number of False Negatives to 13.

Another way to improve the model is to assign weights to the type of errors. As we discussed above, classifying the actual "Default" cases as negative (False Negatives) are the most costly. We can assign the highest (critical) weight of 4 to this type of error. We can also assign the weight of 1 to the False Positives error, since those cases are missed opportunities for the bank.
```{r}
matrix_dimensions <- list(c("no", "yes"), c("no", "yes"))
names(matrix_dimensions) <- c("actual", "predicted")
error_cost <- matrix(c(0, 4, 1, 0), nrow = 2, dimnames = matrix_dimensions)
error_cost

# New model with cost parameter incorporated
credit_cost <- C5.0(credit_train[-17], credit_train$default,
                    costs = error_cost)
credit_cost_pred <- predict(credit_cost, credit_test)

CrossTable(credit_test$default, credit_cost_pred,
           prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE,
           dnn = c('actual default', 'predicted default'))

```
Unfortunately, incorporating the cost factor did not improve the model performance.
Most likely in the real world banks would be hesitant to adopt the model due to the low Accuracy rate and significantly high number of False Negatives in the model. One of the factors contributing to that could be the relatively small size of our training set.